import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import urllib3
import ssl
import re
import json
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager
from urllib3.util import ssl_

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_tcx_news(seen_ids):
    """
    H√†m c√†o Techcom Securities (TCX/TCBS).
    - Method: POST (Form Data).
    - Response: JSON ch·ª©a HTML string.
    - Logic: Parse JSON -> L·∫•y HTML -> Parse BeautifulSoup.
    """
    
    current_year = str(datetime.now().year)
    
    # URL Endpoint x·ª≠ l√Ω AJAX
    api_url = "https://www.tcbs.com.vn/wp-content/custom-ajax.php"
    
    # Danh s√°ch Payload c·∫•u h√¨nh cho t·ª´ng m·ª•c
    categories = [
        {
            "name": "C√¥ng b·ªë th√¥ng tin",
            "slug": "cong-bo-thong-tin"
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "slug": "dai-hoi-dong-co-dong"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "slug": "bao-cao-tai-chinh"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "X-Requested-With": "XMLHttpRequest", # Gi·∫£ l·∫≠p AJAX call
        "Origin": "https://www.tcbs.com.vn",
        "Referer": "https://www.tcbs.com.vn/"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t TCX (NƒÉm {current_year}) ---")

    for cat in categories:
        # Qu√©t trang 0 v√† 1 (th∆∞·ªùng AJAX load more t√≠nh t·ª´ 0)
        # V√¨ filter theo nƒÉm 2025 n√™n server ƒë√£ l·ªçc s·∫µn, ta kh√¥ng c·∫ßn loop qu√° nhi·ªÅu trang
        for page in range(2): 
            # C·∫•u tr√∫c Payload chu·∫©n nh∆∞ ·∫£nh b·∫°n g·ª≠i
            payload = {
                "action": "load_more_posts",
                "page": str(page),
                "types": "investor_relations",
                "category_slug": cat["slug"],
                "search_keywords": "",
                "search_year": current_year, # Server t·ª± l·ªçc nƒÉm
                "search_month": "-1"
            }
            
            try:
                # G·ª≠i POST request
                response = session.post(api_url, headers=headers, data=payload, timeout=20, verify=False)
                
                if response.status_code != 200:
                    print(f"[TCX] L·ªói k·∫øt n·ªëi {cat['name']}: {response.status_code}")
                    break
                
                # B∆Ø·ªöC 1: Parse JSON ƒë·ªÉ l·∫•y c·ª•c HTML
                try:
                    json_data = response.json()
                    # D·ª±a v√†o ·∫£nh 4, HTML n·∫±m trong key 'html' ho·∫∑c ƒë√¥i khi tr·∫£ v·ªÅ tr·ª±c ti·∫øp n·∫øu c·∫•u h√¨nh l·∫°
                    # Nh∆∞ng th∆∞·ªùng WordPress tr·∫£ v·ªÅ {"success": true, "html": "..."} ho·∫∑c ch·ªâ {"html": "..."}
                    # Ta l·∫•y linh ƒë·ªông:
                    html_source = json_data.get("html") or json_data.get("data")
                    
                    # N·∫øu h·∫øt tin, html_source s·∫Ω r·ªóng ho·∫∑c l√† chu·ªói ""
                    if not html_source: 
                        break 
                        
                except json.JSONDecodeError:
                    # Tr∆∞·ªùng h·ª£p server tr·∫£ v·ªÅ l·ªói PHP ho·∫∑c string raw
                    print(f"[TCX] Response kh√¥ng ph·∫£i JSON t·∫°i {cat['name']}")
                    break

                # B∆Ø·ªöC 2: D√πng BS4 x·ª≠ l√Ω c·ª•c HTML ƒë√≥
                soup = BeautifulSoup(html_source, 'html.parser')
                
                # Selector d·ª±a tr√™n ·∫£nh 3 (div class="custom-post-item-news")
                items = soup.select('.custom-post-item-news')
                
                if not items:
                    break # Kh√¥ng c√≥ b√†i n√†o
                
                count_in_page = 0
                for item in items:
                    # 1. L·∫•y Link & Title (trong th·∫ª h2 > a)
                    h2_tag = item.find('h2')
                    if not h2_tag: continue
                    
                    a_tag = h2_tag.find('a')
                    if not a_tag: continue
                    
                    link = a_tag.get('href')
                    # Title n·∫±m trong th·∫ª a, Python requests.json() ƒë√£ t·ª± decode unicode (\u...)
                    title = a_tag.get_text(strip=True)
                    
                    if not link: continue
                    
                    # Fix link n·∫øu thi·∫øu domain (ƒë·ªÅ ph√≤ng)
                    if not link.startswith('http'):
                        link = f"https://www.tcbs.com.vn{link}"

                    # 2. L·∫•y Ng√†y (div class="post-date")
                    date_tag = item.select_one('.post-date')
                    date_str = date_tag.get_text(strip=True) if date_tag else current_year
                    
                    # 3. Check tr√πng
                    news_id = link
                    if news_id in seen_ids: continue
                    if any(x['id'] == news_id for x in new_items): continue

                    new_items.append({
                        "source": f"TCX - {cat['name']}",
                        "id": news_id,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    count_in_page += 1
                
                # N·∫øu page n√†y kh√¥ng c√≥ tin n√†o -> D·ª´ng loop
                if count_in_page == 0:
                    break
                    
                time.sleep(0.5)

            except Exception as e:
                print(f"[TCX] L·ªói x·ª≠ l√Ω {cat['name']}: {e}")
                break

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_stb_news(seen_ids):
    """
    H√†m c√†o Sacombank (STB).
    - Link 1 (CBTT): Key 'data' -> 'downloadPath'
    - Link 2 (BCTC): Key 'data' -> 'documents' -> 'urlFinancialReportStatements'
    - Link 3 (ƒêHƒêCƒê): Key 'news' -> 'downloadUrl'
    """
    
    current_year = datetime.now().year
    domain = "https://www.sacombank.com.vn"
    
    # C·∫•u h√¨nh 3 endpoint
    endpoints = [
        {
            "name": "C√¥ng b·ªë th√¥ng tin",
            "url": "https://www.sacombank.com.vn/trang-chu/nha-dau-tu/cong-bo-thong-tin/_jcr_content/root/container/container/shareholdernotice.sacom.shnotice.json",
            "type": "CBTT"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://www.sacombank.com.vn/trang-chu/nha-dau-tu/bao-cao/_jcr_content/root/container/container/reportlisting.sacom.reportlisting.financial.json",
            "type": "FINANCE"
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": "https://www.sacombank.com.vn/trang-chu/nha-dau-tu/dai-hoi-dong-co-dong/_jcr_content/root/container/container/shareholdercongress.sacom.shareholder.shareholder-congress.json",
            "type": "AGM"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t Sacombank (NƒÉm {current_year}) ---")

    for ep in endpoints:
        try:
            # print(f"   >> ƒêang t·∫£i: {ep['name']}...")
            response = session.get(ep['url'], headers=headers, timeout=20, verify=False)
            
            if response.status_code != 200:
                print(f"[STB] L·ªói k·∫øt n·ªëi {ep['name']}: {response.status_code}")
                continue

            json_data = response.json()
            
            # --- X·ª¨ L√ù T·ª™NG LO·∫†I JSON ---
            
            # 1. LO·∫†I CBTT (Shareholder Notice)
            if ep['type'] == "CBTT":
                items = json_data.get("data", [])
                for item in items:
                    title = item.get("title")
                    link = item.get("downloadPath")
                    date_raw = item.get("date") # "Nov 28, 2025, 12:00:00 AM"
                    
                    if not link or not title: continue
                    
                    # Parse ng√†y
                    date_str = str(current_year)
                    if date_raw:
                        try:
                            # Parse format: Nov 28, 2025...
                            dt_obj = datetime.strptime(date_raw.split(",")[0] + ", " + date_raw.split(",")[1], "%b %d, %Y")
                            if dt_obj.year != current_year: continue
                            date_str = dt_obj.strftime("%d/%m/%Y")
                        except: pass
                    
                    # Gh√©p domain
                    full_link = f"{domain}{link}"
                    
                    # L∆∞u
                    news_id = full_link
                    if news_id in seen_ids: continue
                    if any(x['id'] == news_id for x in new_items): continue
                    
                    new_items.append({
                        "source": f"STB - {ep['name']}",
                        "id": news_id,
                        "title": title,
                        "date": date_str,
                        "link": full_link
                    })

            # 2. LO·∫†I FINANCE (B√°o c√°o t√†i ch√≠nh)
            elif ep['type'] == "FINANCE":
                # C·∫•u tr√∫c: data -> list -> documents -> item
                groups = json_data.get("data", [])
                for group in groups:
                    docs = group.get("documents", [])
                    for doc in docs:
                        title = doc.get("reportTitle")
                        link = doc.get("urlFinancialReportStatements")
                        
                        if not link or not title: continue
                        
                        # Lo·∫°i Finance n√†y kh√¥ng c√≥ field date c·ª• th·ªÉ trong item
                        # Ta l·ªçc b·∫±ng ti√™u ƒë·ªÅ ho·∫∑c l·∫•y nƒÉm t·ª´ root (n·∫øu c·∫ßn)
                        # ·ªû ƒë√¢y l·ªçc ti√™u ƒë·ªÅ ch·ª©a nƒÉm hi·ªán t·∫°i cho ch·∫Øc
                        if str(current_year) not in title: continue

                        full_link = f"{domain}{link}"
                        
                        news_id = full_link
                        if news_id in seen_ids: continue
                        if any(x['id'] == news_id for x in new_items): continue
                        
                        new_items.append({
                            "source": f"STB - {ep['name']}",
                            "id": news_id,
                            "title": title,
                            "date": str(current_year),
                            "link": full_link
                        })

            # 3. LO·∫†I AGM (ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng)
            elif ep['type'] == "AGM":
                # C·∫•u tr√∫c: news -> list
                items = json_data.get("news", [])
                for item in items:
                    title = item.get("title")
                    link = item.get("downloadUrl")
                    date_raw = item.get("date") # "Apr 21, 2025..."
                    year_val = item.get("year")
                    
                    if not link or not title: continue
                    
                    # Check nƒÉm
                    if year_val and int(year_val) != current_year:
                        continue
                        
                    # Parse ng√†y hi·ªÉn th·ªã
                    date_str = str(current_year)
                    if date_raw:
                        try:
                            dt_obj = datetime.strptime(date_raw.split(",")[0] + ", " + date_raw.split(",")[1], "%b %d, %Y")
                            date_str = dt_obj.strftime("%d/%m/%Y")
                        except: pass

                    full_link = f"{domain}{link}"
                    
                    news_id = full_link
                    if news_id in seen_ids: continue
                    if any(x['id'] == news_id for x in new_items): continue
                    
                    new_items.append({
                        "source": f"STB - {ep['name']}",
                        "id": news_id,
                        "title": title,
                        "date": date_str,
                        "link": full_link
                    })

            time.sleep(0.5)

        except Exception as e:
            print(f"[STB] L·ªói t·∫°i {ep['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_hvn_news(seen_ids):
    """
    H√†m c√†o Vietnam Airlines (HVN).
    - Endpoint: .asmx WebAPI.
    - Response: { "d": "JSON_STRING" } -> C·∫ßn json.loads 2 l·∫ßn.
    """
    
    current_year = str(datetime.now().year)
    domain = "https://www.vietnamairlines.com"
    
    # C·∫•u h√¨nh 2 endpoint
    endpoints = [
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng (Tin t·ª©c)",
            "url": "https://www.vietnamairlines.com/WebAPI/CD/CDService.asmx/ListNewsWithDate",
            "type": "NEWS",
            # Payload Link 1
            "payload": {
                "id": "{9539FC34-7AE2-44DE-80E2-7CF9D04742F4}",
                "nameLanguage": "vi-VN",
                "currentPage": "1", # L·∫•y trang 1 l√† ƒë·ªß
                "pageSize": "10",   # TƒÉng nh·∫π l√™n 10 ƒë·ªÉ bao qu√°t
                "group": "4",
                "catergoryId": "0",
                "subjectId": "0",
                "sortorder": ""
            }
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh (Download)",
            "url": "https://www.vietnamairlines.com/WebAPI/CD/CDService.asmx/ListDownload",
            "type": "DOWNLOAD",
            # Payload Link 2
            "payload": {
                "id": "{F3056328-8000-4FE9-A779-E537BF70DC14}",
                "nameLanguage": "vi-VN",
                "currentPage": "1",
                "pageSize": "10",
                "group": "4",
                "catergoryId": "0",
                "subjectId": "0"
            }
        }
    ]
    
    # Header quan tr·ªçng cho ASMX
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Content-Type": "application/json; charset=utf-8" 
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t Vietnam Airlines (NƒÉm {current_year}) ---")

    for ep in endpoints:
        try:
            # G·ª≠i request POST v·ªõi payload JSON
            response = session.post(ep['url'], headers=headers, json=ep['payload'], timeout=20, verify=False)
            
            if response.status_code != 200:
                print(f"[HVN] L·ªói k·∫øt n·ªëi {ep['name']}: {response.status_code}")
                continue

            # --- B√ìC T√ÅCH JSON 2 L·ªöP ---
            try:
                # L·ªõp 1: L·∫•y wrapper
                wrapper = response.json()
                inner_json_str = wrapper.get("d")
                
                if not inner_json_str:
                    continue
                    
                # L·ªõp 2: Parse string b√™n trong 'd'
                real_data = json.loads(inner_json_str)
                
            except Exception as e:
                print(f"[HVN] L·ªói parse JSON {ep['name']}: {e}")
                continue

            # --- X·ª¨ L√ù D·ªÆ LI·ªÜU ---
            items = []
            if ep['type'] == "NEWS":
                items = real_data.get("NewsWithDates", [])
            elif ep['type'] == "DOWNLOAD":
                items = real_data.get("DownloadItem", []) # D·ª±a v√†o snippet b·∫°n g·ª≠i
            
            for item in items:
                title = item.get("Title")
                if not title: continue

                # X·ª≠ l√Ω Link & Date t√πy lo·∫°i
                link = ""
                date_str = current_year
                
                if ep['type'] == "NEWS":
                    link = item.get("NewsWithDateLink")
                    # L·∫•y ng√†y t·ª´ CreateDate: "26/06/2025"
                    raw_date = item.get("CreateDate")
                    if raw_date:
                        date_str = raw_date
                        # Filter nƒÉm
                        if str(current_year) not in raw_date:
                            continue
                
                elif ep['type'] == "DOWNLOAD":
                    link = item.get("Link")
                    # Lo·∫°i n√†y kh√¥ng c√≥ field Date trong snippet
                    # Ta filter b·∫±ng c√°ch check Title ho·∫∑c Link c√≥ ch·ª©a "2025" kh√¥ng
                    check_str = (title + str(link)).lower()
                    if str(current_year) not in check_str:
                         # N·∫øu kh√¥ng th·∫•y nƒÉm hi·ªán t·∫°i trong t√™n file/link -> B·ªè qua cho an to√†n
                         continue

                if not link: continue
                
                # Gh√©p domain n·∫øu c·∫ßn
                if not link.startswith("http"):
                    full_link = f"{domain}{link}"
                else:
                    full_link = link
                
                # Check tr√πng
                news_id = full_link
                if news_id in seen_ids: continue
                if any(x['id'] == news_id for x in new_items): continue

                new_items.append({
                    "source": f"HVN - {ep['name']}",
                    "id": news_id,
                    "title": title,
                    "date": date_str,
                    "link": full_link
                })

            time.sleep(0.5)

        except Exception as e:
            print(f"[HVN] Exception t·∫°i {ep['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_gee_news(seen_ids):
    """
    H√†m c√†o Gelex Electric (GEE) - V3 Final.
    - S·ª≠a l·ªói l·∫•y tin 2024: Si·∫øt ch·∫∑t format ng√†y dd-mm-yyyy.
    - S·ª≠a l·ªói BCTC: B·ªè params "?nam=..." ƒë·ªÉ load m·∫∑c ƒë·ªãnh nƒÉm hi·ªán t·∫°i.
    - Map c·ªôt BCTC chu·∫©n: T√™n, Q1, Q2, Q3, Q4.
    """
    
    current_year = str(datetime.now().year)
    
    configs = [
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": "https://gelex-electric.com/doc-cat/tai-lieu-dai-hoi-dong-cd",
            "type": "LIST"
        },
        {
            "name": "C√¥ng b·ªë th√¥ng tin",
            "url": "https://gelex-electric.com/doc-cat/cong-bo-thong-tin-2",
            "type": "LIST"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://gelex-electric.com/doc-cat/bao-cao-tai-chinh",
            "type": "TABLE"
            # ƒê√É B·ªé PARAMS G√ÇY L·ªñI
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t GEE (NƒÉm {current_year}) ---")

    for cfg in configs:
        try:
            # print(f"   >> ƒêang t·∫£i: {cfg['name']}...")
            response = session.get(cfg['url'], headers=headers, timeout=20, verify=False)
            
            if response.status_code != 200:
                print(f"[GEE] L·ªói k·∫øt n·ªëi {cfg['name']}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')

            # ==========================================================
            # PARSER 1: D·∫†NG LIST (Tin t·ª©c, ƒêHƒêCƒê)
            # ==========================================================
            if cfg['type'] == "LIST":
                items = soup.select('.report-item')
                
                for item in items:
                    # 1. L·∫§Y NG√ÄY & L·ªåC C·ª®NG (Quan tr·ªçng)
                    date_tag = item.select_one('.entry-date')
                    date_str = ""
                    if date_tag:
                        raw_date = date_tag.get_text(strip=True) # VD: 15-03-2024
                        
                        # Fix l·ªói l·∫•y nh·∫ßm 2024: Check string tr·ª±c ti·∫øp tr∆∞·ªõc
                        if current_year not in raw_date:
                            continue # B·ªè qua ngay n·∫øu kh√¥ng ch·ª©a "2025"
                            
                        # Format l·∫°i cho ƒë·∫πp
                        date_str = raw_date.replace("-", "/")
                    else:
                        # Kh√¥ng c√≥ ng√†y -> B·ªè qua cho an to√†n
                        continue

                    # 2. L·∫§Y LINK (∆Øu ti√™n link t·∫£i, fallback sang link b√†i vi·∫øt)
                    # Link b√†i vi·∫øt (lu√¥n c√≥)
                    title_a = item.select_one('.title a')
                    if not title_a: continue
                    title_link = title_a.get('href')
                    title = title_a.get_text(strip=True)
                    
                    # Link download (c√≥ th·ªÉ r·ªóng href="")
                    dl_a = item.select_one('.report-item-link a')
                    dl_link = dl_a.get('href') if dl_a else ""

                    # Logic ch·ªçn link: N·∫øu link download x·ªãn (d√†i > 5 k√Ω t·ª±) th√¨ l·∫•y, ko th√¨ l·∫•y link b√†i
                    final_link = dl_link if (dl_link and len(dl_link) > 5) else title_link
                    
                    if not final_link: continue
                    
                    # 3. Check tr√πng
                    if final_link in seen_ids: continue
                    if any(x['id'] == final_link for x in new_items): continue
                    
                    new_items.append({
                        "source": f"GEE - {cfg['name']}",
                        "id": final_link,
                        "title": title,
                        "date": date_str,
                        "link": final_link
                    })

            # ==========================================================
            # PARSER 2: D·∫†NG TABLE (B√°o c√°o t√†i ch√≠nh)
            # ==========================================================
            elif cfg['type'] == "TABLE":
                table = soup.select_one('.table-report')
                
                # N·∫øu kh√¥ng c√≥ b·∫£ng (do ch∆∞a c√≥ tin 2025 ho·∫∑c l·ªói load) -> Skip
                if not table: 
                    # print(f"[GEE] Kh√¥ng th·∫•y b·∫£ng t·∫°i {cfg['name']}")
                    continue

                rows = table.select('tr')
                current_group = "B√°o c√°o"
                
                # Mapping c·ªôt: Index 0 l√† T√™n, 1->4 l√† Qu√Ω
                quarter_map = {1: "Q1", 2: "Q2", 3: "Q3", 4: "Q4"}
                
                for row in rows:
                    # A. D√≤ng Nh√≥m (Parent) - VD: B√°o c√°o t√†i ch√≠nh
                    parent_td = row.select_one('.parent')
                    if parent_td:
                        current_group = parent_td.get_text(strip=True)
                        continue
                    
                    # B. D√≤ng Con (Child) - VD: B√°o c√°o Ri√™ng
                    # Class 'quatar' (sai ch√≠nh t·∫£) ch·ª©a t√™n d√≤ng
                    name_td = row.select_one('.quatar')
                    if not name_td: continue
                    
                    row_name = name_td.get_text(strip=True)
                    
                    # L·∫•y t·∫•t c·∫£ c√°c √¥ td tr·ª±c ti·∫øp c·ªßa d√≤ng n√†y
                    cells = row.find_all('td', recursive=False)
                    
                    for idx, cell in enumerate(cells):
                        if idx == 0: continue # C·ªôt 0 l√† t√™n, b·ªè qua
                        
                        # T√¨m link trong √¥ (class quarter)
                        a_tag = cell.find('a')
                        if not a_tag: continue
                        
                        link = a_tag.get('href')
                        if not link: continue
                        
                        # L·∫•y ng√†y (span.meta-date)
                        meta_date = cell.select_one('.meta-date')
                        date_text = meta_date.get_text(strip=True) if meta_date else ""
                        
                        # Filter nƒÉm c·ª©ng: Ph·∫£i c√≥ "2025"
                        if current_year not in date_text: continue
                        
                        # T·∫°o ti√™u ƒë·ªÅ: B√°o c√°o t√†i ch√≠nh - B√°o C√°o Ri√™ng - Q2 2025
                        q_name = quarter_map.get(idx, "")
                        full_title = f"{current_group} - {row_name} {q_name} {current_year}"
                        
                        # Check tr√πng
                        if link in seen_ids: continue
                        if any(x['id'] == link for x in new_items): continue
                        
                        new_items.append({
                            "source": f"GEE - {cfg['name']}",
                            "id": link,
                            "title": full_title,
                            "date": date_text,
                            "link": link
                        })

            time.sleep(0.5)

        except Exception as e:
            print(f"[GEE] L·ªói ngo·∫°i l·ªá t·∫°i {cfg['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_vre_news(seen_ids):
    """
    H√†m c√†o Vincom Retail (VRE).
    - Parser LIST: X·ª≠ l√Ω ƒêHƒêCƒê & CBTT (h6 > a, time).
    - Parser TABLE: X·ª≠ l√Ω BCTC (table > tr > td).
    """
    
    current_year = str(datetime.now().year)
    
    configs = [
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://ir.vincom.com.vn/bao-cao-tai-chinh-va-tom-tat-ket-qua-kinh-doanh/",
            "type": "TABLE"
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": "https://ir.vincom.com.vn/cong-bo-thong-tin/dai-hoi-dong-co-dong/",
            "type": "LIST"
        },
        {
            "name": "C√¥ng b·ªë th√¥ng tin",
            "url": "https://ir.vincom.com.vn/cong-bo-thong-tin/cong-bo-thong-tin-vi/",
            "type": "LIST"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t VRE (NƒÉm {current_year}) ---")

    for cfg in configs:
        try:
            # print(f"   >> ƒêang t·∫£i: {cfg['name']}...")
            response = session.get(cfg['url'], headers=headers, timeout=20, verify=False)
            
            if response.status_code != 200:
                print(f"[VRE] L·ªói k·∫øt n·ªëi {cfg['name']}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')

            # ==========================================================
            # PARSER 1: D·∫†NG LIST (ƒêHƒêCƒê, CBTT)
            # C·∫•u tr√∫c: .column > .item > h6 > a
            # ==========================================================
            if cfg['type'] == "LIST":
                # T√¨m t·∫•t c·∫£ kh·ªëi tin (.item)
                items = soup.select('.item')
                
                for item in items:
                    # 1. Ti√™u ƒë·ªÅ & Link (h6 > a)
                    h6_tag = item.select_one('h6 a')
                    if not h6_tag: continue
                    
                    title = h6_tag.get_text(strip=True)
                    link = h6_tag.get('href')
                    if not link: continue

                    # 2. Ng√†y th√°ng (time tag)
                    # HTML: <time ...>26/8/2025</time>
                    date_tag = item.select_one('time')
                    date_str = ""
                    if date_tag:
                        date_str = date_tag.get_text(strip=True)
                    
                    # N·∫øu kh√¥ng c√≥ th·∫ª time, t√¨m div ch·ª©a ng√†y (fallback)
                    if not date_str:
                        meta_div = item.select_one('.post-meta')
                        if meta_div: date_str = meta_div.get_text(strip=True)

                    # L·ªçc nƒÉm (VRE d√πng ƒë·ªãnh d·∫°ng dd/mm/yyyy)
                    if current_year not in date_str: continue

                    # 3. Chu·∫©n h√≥a & L∆∞u
                    if not link.startswith('http'):
                        link = f"https://ir.vincom.com.vn{link}"
                    
                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue
                    
                    new_items.append({
                        "source": f"VRE - {cfg['name']}",
                        "id": link,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })

            # ==========================================================
            # PARSER 2: D·∫†NG TABLE (B√°o c√°o t√†i ch√≠nh)
            # C·∫•u tr√∫c: table > tr > td (Link v√† Date n·∫±m chung trong td)
            # ==========================================================
            elif cfg['type'] == "TABLE":
                # T√¨m b·∫£ng
                # C√≥ th·ªÉ t√¨m table chung v√¨ trang n√†y ch·ªâ c√≥ 1 b·∫£ng ch√≠nh
                table = soup.select_one('table')
                if not table: continue

                rows = table.select('tr')
                current_group = "B√°o c√°o"
                
                # Mapping c·ªôt: Index 0 l√† T√™n b√°o c√°o, 1->4 l√† Q1->Q4
                quarter_map = {1: "Q1", 2: "Q2", 3: "Q3", 4: "Q4"}
                
                for row in rows:
                    # A. X√°c ƒë·ªãnh d√≤ng Ti√™u ƒë·ªÅ nh√≥m (VD: B√ÅO C√ÅO T√ÄI CH√çNH)
                    # D·ª±a v√†o style background-color ho·∫∑c th·∫ª b/strong trong c·ªôt ƒë·∫ßu
                    first_td = row.select_one('td')
                    if first_td:
                        style = first_td.get('style', '').lower()
                        # M√†u ƒë·ªè ƒë·∫∑c tr∆∞ng c·ªßa Vincom (#d33039)
                        if 'background-color' in style or 'bold' in style:
                            text = first_td.get_text(strip=True)
                            if len(text) > 3: # Tr√°nh l·∫•y nh·∫ßm d√≤ng r√°c
                                current_group = text
                                continue # B·ªè qua d√≤ng ti√™u ƒë·ªÅ n√†y
                    
                    # B. X√°c ƒë·ªãnh d√≤ng D·ªØ li·ªáu
                    # C·ªôt 1 l√† t√™n lo·∫°i b√°o c√°o (VD: B√°o C√°o H·ª£p Nh·∫•t...)
                    cells = row.find_all('td', recursive=False)
                    if not cells: continue
                    
                    row_name = cells[0].get_text(strip=True)
                    if not row_name: continue

                    # Duy·ªát c√°c c·ªôt Qu√Ω (t·ª´ index 1 tr·ªü ƒëi)
                    for idx, cell in enumerate(cells):
                        if idx == 0: continue
                        
                        # T√¨m link trong √¥
                        a_tag = cell.find('a')
                        if not a_tag: continue
                        
                        link = a_tag.get('href')
                        if not link: continue
                        
                        # T√¨m ng√†y: th∆∞·ªùng n·∫±m trong th·∫ª div ho·∫∑c ngay sau th·∫ª p ch·ª©a link
                        # HTML: <div>28/08/2025</div>
                        # L·∫•y t·∫•t c·∫£ text trong √¥, tr·ª´ text c·ªßa link
                        cell_text = cell.get_text(" ", strip=True)
                        link_text = a_tag.get_text(strip=True)
                        date_text = cell_text.replace(link_text, "").strip() # Lo·∫°i b·ªè ch·ªØ "PDF"
                        
                        # L·ªçc nƒÉm 2025
                        if current_year not in date_text: continue
                        
                        # T·∫°o ti√™u ƒë·ªÅ
                        q_name = quarter_map.get(idx, "")
                        full_title = f"{current_group} - {row_name} {q_name} {current_year}"
                        
                        # Chu·∫©n h√≥a link
                        if not link.startswith('http'):
                            link = f"https://ir.vincom.com.vn{link}"

                        # Check tr√πng
                        if link in seen_ids: continue
                        if any(x['id'] == link for x in new_items): continue
                        
                        new_items.append({
                            "source": f"VRE - {cfg['name']}",
                            "id": link,
                            "title": full_title,
                            "date": date_text, # L·∫•y chu·ªói ng√†y t√¨m ƒë∆∞·ª£c
                            "link": link
                        })

            time.sleep(0.5)

        except Exception as e:
            print(f"[VRE] L·ªói ngo·∫°i l·ªá t·∫°i {cfg['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_shb_news(seen_ids):
    """
    H√†m c√†o SHB (Ng√¢n h√†ng S√†i G√≤n - H√† N·ªôi).
    - C·∫•u tr√∫c: div.item_ndt -> div.title -> a -> span.time
    - X·ª≠ l√Ω ng√†y th√°ng d·∫°ng (dd-mm-yyyy) n·∫±m trong ngo·∫∑c ƒë∆°n.
    """
    
    current_year = str(datetime.now().year)
    
    # C·∫•u h√¨nh danh m·ª•c v√† URL template cho ph√¢n trang
    configs = [
        {
            "name": "C√¥ng b·ªë th√¥ng tin",
            "base_url": "https://www.shb.com.vn/category/nha-dau-tu/cong-bo-thong-tin/"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "base_url": "https://www.shb.com.vn/category/nha-dau-tu/bao-cao-tai-chinh/"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t SHB (NƒÉm {current_year}) ---")

    for cfg in configs:
        # Qu√©t 2 trang ƒë·∫ßu (WordPress th∆∞·ªùng ph√¢n trang ki·ªÉu /page/2/)
        for page in range(1, 2):
            if page == 1:
                url = cfg['base_url']
            else:
                url = f"{cfg['base_url']}page/{page}/"
                
            try:
                # print(f"   >> ƒêang t·∫£i: {cfg['name']} - Trang {page}...")
                response = session.get(url, headers=headers, timeout=20, verify=False)
                
                if response.status_code != 200:
                    print(f"[SHB] L·ªói k·∫øt n·ªëi {cfg['name']}: {response.status_code}")
                    break

                soup = BeautifulSoup(response.text, 'html.parser')
                
                # T√¨m c√°c kh·ªëi tin (item_ndt)
                items = soup.select('div.item_ndt')
                
                if not items:
                    break # H·∫øt tin -> D·ª´ng
                
                count_in_page = 0
                for item in items:
                    # 1. T√¨m kh·ªëi Title
                    title_div = item.select_one('.title')
                    if not title_div: continue
                    
                    a_tag = title_div.find('a')
                    if not a_tag: continue
                    
                    link = a_tag.get('href')
                    if not link: continue
                    
                    # 2. X·ª≠ l√Ω Ng√†y th√°ng (span.time)
                    # Format: (22-10-2025)
                    time_span = a_tag.select_one('span.time')
                    date_str = ""
                    
                    if time_span:
                        raw_time = time_span.get_text(strip=True)
                        # Lo·∫°i b·ªè ngo·∫∑c ƒë∆°n ()
                        clean_time = raw_time.replace('(', '').replace(')', '').strip()
                        
                        try:
                            # Parse ng√†y
                            dt_obj = datetime.strptime(clean_time, "%d-%m-%Y")
                            if str(dt_obj.year) != current_year:
                                continue # B·ªè qua tin c≈©
                            date_str = clean_time.replace("-", "/")
                        except:
                            pass
                            
                    # N·∫øu kh√¥ng parse ƒë∆∞·ª£c ng√†y ho·∫∑c kh√¥ng c√≥ th·∫ª time -> Ki·ªÉm tra title/link fallback
                    if not date_str:
                        if current_year not in a_tag.get_text():
                            continue
                        date_str = current_year

                    # 3. L·∫•y Ti√™u ƒë·ªÅ s·∫°ch (lo·∫°i b·ªè ph·∫ßn ng√†y th√°ng trong th·∫ª a)
                    # V√¨ SHB nh√©t span.time v√†o trong th·∫ª a, n√™n get_text() s·∫Ω l·∫•y c·∫£ ng√†y
                    # Ta c·∫ßn remove text c·ªßa time_span ƒëi
                    full_text = a_tag.get_text(strip=True)
                    if time_span:
                        time_text = time_span.get_text(strip=True)
                        title = full_text.replace(time_text, "").strip()
                    else:
                        title = full_text

                    # 4. Check tr√πng
                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue
                    
                    new_items.append({
                        "source": f"SHB - {cfg['name']}",
                        "id": link,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    count_in_page += 1
                
                # N·∫øu trang n√†y kh√¥ng c√≥ tin m·ªõi n√†o -> D·ª´ng loop
                if count_in_page == 0:
                    break
                
                time.sleep(0.5)

            except Exception as e:
                print(f"[SHB] L·ªói ngo·∫°i l·ªá t·∫°i {cfg['name']}: {e}")
                continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_bsr_news(seen_ids):
    """
    H√†m c√†o BSR (L·ªçc h√≥a d·∫ßu B√¨nh S∆°n).
    - Ch·ªâ qu√©t trang 1.
    - L·ªçc nhanh b·∫±ng thu·ªôc t√≠nh 'data-year' c·ªßa th·∫ª tr.
    - L·∫•y link t·∫£i tr·ª±c ti·∫øp t·ª´ th·∫ª a c√≥ title="T·∫£i v·ªÅ".
    """
    
    current_year = str(datetime.now().year)
    domain = "https://bsr.com.vn"
    
    configs = [
        {
            "name": "ƒê·∫°i h·ªôi c·ªï ƒë√¥ng",
            "url": "https://bsr.com.vn/dai-hoi-co-dong"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://bsr.com.vn/bao-cao"
        },
        {
            "name": "C√¥ng b·ªë th√¥ng tin kh√°c",
            "url": "https://bsr.com.vn/cong-bo-thong-tin-khac"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t BSR (NƒÉm {current_year}) ---")

    for cfg in configs:
        try:
            # Ch·ªâ request trang ƒë·∫ßu (Page 1)
            response = session.get(cfg['url'], headers=headers, timeout=20, verify=False)
            
            if response.status_code != 200:
                print(f"[BSR] L·ªói k·∫øt n·ªëi {cfg['name']}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m t·∫•t c·∫£ c√°c d√≤ng d·ªØ li·ªáu (tr.document-item)
            rows = soup.select('tr.document-item')
            
            if not rows:
                continue

            for row in rows:
                # 1. L·ªåC NƒÇM C·ª∞C NHANH
                # Web BSR c√≥ thu·ªôc t√≠nh data-year="2025" ngay tr√™n th·∫ª tr
                data_year = row.get('data-year')
                if data_year and data_year != current_year:
                    continue # B·ªè qua ngay n·∫øu kh√¥ng ph·∫£i nƒÉm nay
                
                # N·∫øu kh√¥ng c√≥ data-year (ph√≤ng h·ªù), check c·ªôt ng√†y
                cols = row.find_all('td')
                if len(cols) < 3: continue
                
                # C·ªôt 1: Ng√†y (Index 1) - VD: 30/10/2025 13:02
                date_text = cols[1].get_text(strip=True)
                if current_year not in date_text:
                    continue
                
                # Format l·∫°i ng√†y: l·∫•y ph·∫ßn ƒë·∫ßu dd/mm/yyyy
                date_str = date_text.split(" ")[0] if " " in date_text else date_text

                # 2. L·∫§Y TI√äU ƒê·ªÄ
                # N·∫±m trong th·∫ª p.document-title ·ªü c·ªôt 0
                title_tag = row.select_one('.document-title')
                title = title_tag.get_text(strip=True) if title_tag else "T√†i li·ªáu BSR"

                # 3. L·∫§Y LINK T·∫¢I
                # Trong c·ªôt cu·ªëi c√πng, t√¨m th·∫ª a c√≥ title="T·∫£i v·ªÅ" ho·∫∑c ch·ª©a "get_file"
                # ∆Øu ti√™n t√¨m th·∫ª c√≥ thu·ªôc t√≠nh download ho·∫∑c title="T·∫£i v·ªÅ"
                download_a = row.select_one('a[title="T·∫£i v·ªÅ"]')
                
                # N·∫øu kh√¥ng th·∫•y, t√¨m th·∫ª a b·∫•t k·ª≥ ch·ª©a link get_file
                if not download_a:
                    download_a = row.select_one('a[href*="get_file"]')
                
                if not download_a: continue
                
                link = download_a.get('href')
                if not link or "javascript" in link: continue

                # Chu·∫©n h√≥a link (th∆∞·ªùng BSR d√πng link t∆∞∆°ng ƒë·ªëi /c/document_library...)
                if not link.startswith('http'):
                    link = f"{domain}{link}"

                # 4. CHECK TR√ôNG & L∆ØU
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue
                
                new_items.append({
                    "source": f"BSR - {cfg['name']}",
                    "id": link,
                    "title": title,
                    "date": date_str,
                    "link": link
                })

            time.sleep(0.5)

        except Exception as e:
            print(f"[BSR] L·ªói ngo·∫°i l·ªá t·∫°i {cfg['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_bcm_news(seen_ids):
    """
    H√†m c√†o Becamex (BCM).
    - C·∫•u tr√∫c chung cho c·∫£ 4 m·ª•c: div.shareholder-item
    - X·ª≠ l√Ω ng√†y ti·∫øng Vi·ªát: "02 Th√°ng 12, 2025"
    """
    
    current_year = str(datetime.now().year)
    
    configs = [
        {
            "name": "C√¥ng b·ªë th√¥ng tin",
            "url": "https://becamex.com.vn/quan-he-co-dong/cong-bo-thong-tin/"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://becamex.com.vn/quan-he-co-dong/bao-cao-tai-chinh/"
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": "https://becamex.com.vn/quan-he-co-dong/dai-hoi-dong-co-dong/"
        },
        {
            "name": "Th√¥ng tin c·ªï ƒë√¥ng",
            "url": "https://becamex.com.vn/quan-he-co-dong/thong-tin-co-dong/"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t BCM (NƒÉm {current_year}) ---")

    for cfg in configs:
        try:
            # Ch·ªâ qu√©t trang ƒë·∫ßu
            response = session.get(cfg['url'], headers=headers, timeout=20, verify=False)
            
            if response.status_code != 200:
                print(f"[BCM] L·ªói k·∫øt n·ªëi {cfg['name']}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m c√°c kh·ªëi tin (shareholder-item)
            items = soup.select('div.shareholder-item')
            
            if not items:
                continue

            for item in items:
                # 1. X·ª¨ L√ù NG√ÄY TH√ÅNG
                # T√¨m th·∫ª p ch·ª©a ng√†y (th∆∞·ªùng l√† th·∫ª p ƒë·∫ßu ti√™n trong item)
                p_tags = item.find_all('p')
                date_str = ""
                
                for p in p_tags:
                    text = p.get_text(strip=True)
                    # Format: "02 Th√°ng 12, 2025"
                    if "Th√°ng" in text and "," in text:
                        # Chu·∫©n h√≥a chu·ªói ng√†y
                        clean_date = text.replace("Th√°ng", "").replace(",", "").strip() # -> "02  12  2025"
                        
                        # X·ª≠ l√Ω kho·∫£ng tr·∫Øng th·ª´a
                        parts = clean_date.split()
                        if len(parts) == 3:
                            day, month, year = parts
                            if year != current_year:
                                break # Kh√¥ng ph·∫£i nƒÉm nay -> D·ª´ng check item n√†y
                            date_str = f"{day}/{month}/{year}"
                            break # ƒê√£ t√¨m th·∫•y ng√†y h·ª£p l·ªá
                
                # N·∫øu kh√¥ng t√¨m th·∫•y ng√†y nƒÉm nay -> B·ªè qua
                if not date_str:
                    continue

                # 2. L·∫§Y LINK V√Ä TITLE
                # T√¨m th·∫ª h2 > a
                h2_tag = item.select_one('h2 a')
                if not h2_tag: continue
                
                link = h2_tag.get('href')
                title = h2_tag.get_text(strip=True)
                
                if not link: continue
                
                # 3. CHECK TR√ôNG & L∆ØU
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue
                
                new_items.append({
                    "source": f"BCM - {cfg['name']}",
                    "id": link,
                    "title": title,
                    "date": date_str,
                    "link": link
                })

            time.sleep(0.5)

        except Exception as e:
            print(f"[BCM] L·ªói ngo·∫°i l·ªá t·∫°i {cfg['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_sab_news(seen_ids):
    """
    H√†m c√†o SABECO (SAB) - Fix l·ªói thi·∫øu BCTC c√°c qu√Ω c≈©.
    - Duy·ªát qua T·∫§T C·∫¢ c√°c kh·ªëi .financy-report (thay v√¨ ch·ªâ kh·ªëi ƒë·∫ßu ti√™n).
    """
    
    current_year = str(datetime.now().year)
    domain = "https://www.sabeco.com.vn"
    
    configs = [
        {
            "name": "C√¥ng b·ªë th√¥ng tin",
            "url": f"https://www.sabeco.com.vn/co-dong/cong-bo-thong-tin/{current_year}"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": f"https://www.sabeco.com.vn/co-dong/bao-cao-tai-chinh/{current_year}-2"
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": f"https://www.sabeco.com.vn/co-dong/dai-hoi-dong-co-dong/{current_year}-4"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t SABECO (NƒÉm {current_year}) ---")

    for cfg in configs:
        try:
            response = session.get(cfg['url'], headers=headers, timeout=20, verify=False)
            
            if response.status_code != 200:
                print(f"[SAB] L·ªói k·∫øt n·ªëi {cfg['name']}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # --- FIX: L·∫•y T·∫§T C·∫¢ c√°c kh·ªëi b√°o c√°o ---
            # M·ªói kh·ªëi t∆∞∆°ng ·ª©ng v·ªõi 1 Qu√Ω ho·∫∑c 1 K·ª≥ (B√°n ni√™n, NƒÉm)
            report_blocks = soup.select('.financy-report')
            
            if not report_blocks:
                # print(f"[SAB] Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu t·∫°i {cfg['name']}")
                continue
            
            for block in report_blocks:
                # L·∫•y danh s√°ch tin trong t·ª´ng kh·ªëi
                list_items = block.select('li')
                
                for li in list_items:
                    # 1. T√¨m Link & Title
                    a_tag = li.find('a')
                    if not a_tag: continue
                    
                    link = a_tag.get('href')
                    if not link: continue
                    
                    title = a_tag.get_text(strip=True)
                    
                    # 2. X·ª≠ l√Ω Ng√†y th√°ng (Text n·∫±m ngo√†i th·∫ª a)
                    # N·ªôi dung li: <a...>Ti√™u ƒë·ªÅ</a> (25/07/2025)
                    full_text = li.get_text(strip=True)
                    date_str = ""
                    
                    # Regex b·∫Øt chu·ªói ng√†y trong ngo·∫∑c
                    match = re.search(r'\((\d{1,2}/\d{1,2}/\d{4})\)', full_text)
                    if match:
                        date_str = match.group(1)
                    else:
                        # Fallback: T√¨m ng√†y d·∫°ng dd/mm/yyyy b·∫•t k·ª≥ trong text
                        match_any = re.search(r'(\d{1,2}/\d{1,2}/\d{4})', full_text)
                        if match_any:
                            date_str = match_any.group(1)
                    
                    # Filter nƒÉm
                    if current_year not in date_str: continue

                    # 3. Chu·∫©n h√≥a & L∆∞u
                    if not link.startswith('http'):
                        link = f"{domain}{link}"
                    
                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue
                    
                    new_items.append({
                        "source": f"SAB - {cfg['name']}",
                        "id": link,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })

            time.sleep(0.5)

        except Exception as e:
            print(f"[SAB] L·ªói ngo·∫°i l·ªá t·∫°i {cfg['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_ssi_news(seen_ids):
    """
    H√†m c√†o SSI (Ch·ª©ng kho√°n SSI).
    - Ph·∫ßn 1: B√°o c√°o t√†i ch√≠nh (D·ª±a tr√™n div class chart__content__item).
    - Ph·∫ßn 2: L·ªãch s·ª≠ c·ªï t·ª©c (D·ª±a tr√™n Table).
    """
    
    current_year = datetime.now().year
    # current_year = 2024 # Uncomment d√≤ng n√†y n·∫øu mu·ªën test v·ªõi d·ªØ li·ªáu nƒÉm c≈©
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t SSI (NƒÉm {current_year}) ---")

    # ==========================================================
    # PH·∫¶N 1: B√ÅO C√ÅO T√ÄI CH√çNH
    # ==========================================================
    bctc_url = "https://www.ssi.com.vn/quan-he-nha-dau-tu/bao-cao-tai-chinh"
    
    # SSI c√≥ param l·ªçc nƒÉm, ta t·∫≠n d·ª•ng lu√¥n
    params = {
        "year": current_year 
    }

    try:
        # print(f"   >> Qu√©t BCTC...")
        response = session.get(bctc_url, headers=headers, params=params, timeout=20, verify=False)
        soup = BeautifulSoup(response.text, 'html.parser')

        # T√¨m c√°c item theo class trong ·∫£nh source code
        items = soup.select('.chart__content__item')
        
        for item in items:
            # 1. L·∫•y Ti√™u ƒë·ªÅ
            title_tag = item.select_one('.chart__content__item__desc p')
            if not title_tag: continue
            title = title_tag.get_text(strip=True)
            
            # 2. L·∫•y Link
            link_tag = item.select_one('.chart__content__item__time a')
            if not link_tag: continue
            link = link_tag.get('href')
            
            if not link: continue
            
            # Chu·∫©n h√≥a link (SSI th∆∞·ªùng ƒë·ªÉ link t∆∞∆°ng ƒë·ªëi /upload/...)
            if not link.startswith('http'):
                link = f"https://www.ssi.com.vn{link}"

            # 3. Ki·ªÉm tra nƒÉm trong ti√™u ƒë·ªÅ (Double check)
            if str(current_year) not in title:
                continue

            # 4. Check tr√πng
            if link in seen_ids: continue
            if any(x['id'] == link for x in new_items): continue

            new_items.append({
                "source": "SSI - BCTC",
                "id": link,
                "title": title,
                "date": str(current_year),
                "link": link
            })

    except Exception as e:
        print(f"[SSI] L·ªói BCTC: {e}")


    # ==========================================================
    # PH·∫¶N 2: L·ªäCH S·ª¨ C·ªî T·ª®C (D·∫°ng B·∫£ng)
    # ==========================================================
    div_url = "https://www.ssi.com.vn/quan-he-nha-dau-tu/lich-su-co-tuc"
    
    try:
        # print(f"   >> Qu√©t L·ªãch s·ª≠ c·ªï t·ª©c...")
        response = session.get(div_url, headers=headers, timeout=20, verify=False)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # T√¨m b·∫£ng
        table = soup.select_one('table.table')
        if table:
            rows = table.find_all('tr')
            
            # B·ªè qua d√≤ng ti√™u ƒë·ªÅ ƒë·∫ßu ti√™n
            for row in rows[1:]:
                cols = row.find_all('td')
                if len(cols) < 7: continue # ƒê·∫£m b·∫£o ƒë·ªß c·ªôt
                
                # C·∫•u tr√∫c c·ªôt theo ·∫£nh:
                # [0] NƒÉm | [1] TG | [2] T·ª∑ l·ªá | [3] GDKHQ | [4] ƒêKCC | [5] Thanh to√°n | [6] H√¨nh th·ª©c
                
                ex_date_raw = cols[3].get_text(strip=True) # Ng√†y giao d·ªãch kh√¥ng h∆∞·ªüng quy·ªÅn
                content_type = cols[6].get_text(strip=True) # Ti·ªÅn m·∫∑t / C·ªï phi·∫øu
                rate = cols[2].get_text(strip=True) # T·ª∑ l·ªá
                
                # Parse ng√†y GDKHQ (dd/mm/yyyy)
                try:
                    ex_date = datetime.strptime(ex_date_raw, "%d/%m/%Y")
                    
                    # LOGIC QUAN TR·ªåNG: Ch·ªâ l·∫•y n·∫øu GDKHQ n·∫±m trong nƒÉm hi·ªán t·∫°i
                    if ex_date.year != current_year:
                        continue
                        
                    date_str = ex_date.strftime("%d/%m/%Y")
                    
                    title = f"Th√¥ng b√°o tr·∫£ c·ªï t·ª©c {content_type} - T·ª∑ l·ªá {rate} (GDKHQ: {date_str})"
                    
                    # T·∫°o ID gi·∫£
                    fake_id = f"SSI_DIV_{ex_date_raw}_{content_type}"
                    link = div_url # Tr·ªè v·ªÅ trang b·∫£ng
                    
                    if fake_id in seen_ids: continue
                    if any(x['id'] == fake_id for x in new_items): continue

                    new_items.append({
                        "source": "SSI - C·ªï T·ª©c",
                        "id": fake_id,
                        "title": title,
                        "date": date_str,
                        "link": link
                    })
                    
                except ValueError:
                    continue 

    except Exception as e:
        print(f"[SSI] L·ªói C·ªï t·ª©c: {e}")

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_vib_news(seen_ids):
    current_year = datetime.now().year
    domain = "https://www.vib.com.vn"
    
    # --- C·∫§U H√åNH HEADERS & COOKIES (Gi·ªØ nguy√™n t·ª´ cURL c≈©) ---
    headers = {
        "accept": "text/html, */*; q=0.01",
        "accept-language": "en-US,en;q=0.9,vi;q=0.8",
        "priority": "u=1, i",
        "referer": "https://www.vib.com.vn/vn/nha-dau-tu",
        "sec-ch-ua": '"Microsoft Edge";v="143", "Chromium";v="143", "Not A(Brand";v="24"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "empty",
        "sec-fetch-mode": "cors",
        "sec-fetch-site": "same-origin",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36 Edg/143.0.0.0",
        "x-requested-with": "XMLHttpRequest"
    }
    
    cookies = {
        "route": "c26a0b557457a2502d35448a2f46e3eb",
        "JSESSIONID": "0000B4eA_vZ-FKVb6ZnnQtx4c51:1evrkq9vn" 
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())
    
    # Danh s√°ch t√™n mi·ªÅn r√°c c·∫ßn lo·∫°i b·ªè (Footer links)
    JUNK_DOMAINS = ['facebook.com', 'youtube.com', 'linkedin.com', 'google.com', 'apple.com', 'goo.gl']

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t VIB (Smart Filter - NƒÉm {current_year}) ---")

    # ==========================================================
    # PH·∫¶N 1: B√ÅO C√ÅO T√ÄI CH√çNH (QU√ù 1 -> 4)
    # ==========================================================
    bctc_cmpnt_id = "242afeb3-0b0e-4413-a11a-86ab453adc26"
    base_bctc = f"https://www.vib.com.vn/wps/wcm/connect/vib-vevib-vn/sa-homepage/shareholder/thong-tin-tai-chinh/{current_year}/bao-cao-quy-{{}}"
    
    for q in range(1, 5):
        current_time = int(time.time())
        url = f"{base_bctc.format(q)}?source=library&srv=cmpnt&cmpntid={bctc_cmpnt_id}&time={current_time}"
        
        try:
            response = session.get(url, headers=headers, cookies=cookies, timeout=20, verify=False)
            
            if response.status_code != 200 or len(response.text) < 100:
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # --- CHI·∫æN THU·∫¨T L·ªåC M·ªöI ---
            # 1. T√¨m ch√≠nh x√°c th·∫ª a c√≥ class "file-link" (nh∆∞ ·∫£nh image_adc15e.png)
            links = soup.select('a.file-link')
            
            # 2. N·∫øu kh√¥ng th·∫•y, fallback t√¨m th·∫ª a n·∫±m trong h4 (nh∆∞ ·∫£nh image_adbe79.png)
            if not links:
                links = [h4.find('a') for h4 in soup.select('h4') if h4.find('a')]
            
            # 3. N·∫øu v·∫´n kh√¥ng th·∫•y, t√¨m th·∫ª a c√≥ path ch·ª©a "/vib-vevib-vn/" (Link n·ªôi b·ªô)
            if not links:
                links = soup.select('a[path^="/vib-vevib-vn/"]')

            found_in_quarter = 0
            for a_tag in links:
                # L·∫•y Link
                path = a_tag.get('href') or a_tag.get('path')
                if not path: continue
                
                # --- L·ªåC R√ÅC QUAN TR·ªåNG ---
                # N·∫øu link ch·ª©a domain r√°c -> B·ªè qua ngay
                if any(junk in path.lower() for junk in JUNK_DOMAINS):
                    continue
                
                # Link ph·∫£i c√≥ ƒë·ªô d√†i nh·∫•t ƒë·ªãnh v√† kh√¥ng ph·∫£i javascript
                if len(path) < 5 or "javascript" in path: continue

                if not path.startswith('http'):
                    full_link = f"{domain}{path}"
                else:
                    full_link = path
                
                title = a_tag.get_text(strip=True)
                
                # L·∫•y Ng√†y (logic c≈© v·∫´n t·ªët)
                date_str = str(current_year)
                date_tag = a_tag.find_next_sibling('i')
                if not date_tag and a_tag.parent: date_tag = a_tag.parent.find('i')
                    
                if date_tag:
                    raw_date = date_tag.get('date-created') or date_tag.get_text(strip=True)
                    if raw_date:
                        try:
                            clean_date = raw_date[:10].replace('-', '/')
                            d_obj = datetime.strptime(clean_date, "%Y/%m/%d")
                            if d_obj.year != current_year: continue
                            date_str = d_obj.strftime("%d/%m/%Y")
                        except: pass

                # Check tr√πng
                if full_link in seen_ids: continue
                if any(x['id'] == full_link for x in new_items): continue

                new_items.append({
                    "source": f"VIB - BCTC Q{q}",
                    "id": full_link,
                    "title": title,
                    "date": date_str,
                    "link": full_link
                })
                found_in_quarter += 1
                
            # print(f"   > Qu√Ω {q}: T√¨m th·∫•y {found_in_quarter} file h·ª£p l·ªá.")

        except Exception as e:
            continue

    # ==========================================================
    # PH·∫¶N 2: TIN KH√ÅC (ƒêHƒêCƒê & C·ªî T·ª®C)
    # ==========================================================
    other_targets = [
        {
            "name": "ƒêHƒêCƒê",
            "url_base": "https://www.vib.com.vn/wps/wcm/connect/vib-vevib-vn/sa-homepage/shareholder/tin-co-dong/thong-tin-dai-hoi-co-dong",
            "cmpntid": "712752d0-d846-46dd-a6ce-c2a63d09ff86"
        },
        {
            "name": "C·ªï t·ª©c",
            "url_base": "https://www.vib.com.vn/wps/wcm/connect/vib-vevib-vn/sa-homepage/shareholder/tin-co-dong/lich-su-tra-co-tuc-bang-tien",
            "cmpntid": "712752d0-d846-46dd-a6ce-c2a63d09ff86"
        }
    ]

    for target in other_targets:
        current_time = int(time.time())
        url = f"{target['url_base']}?source=library&srv=cmpnt&cmpntid={target['cmpntid']}&time={current_time}"
        
        try:
            response = session.get(url, headers=headers, cookies=cookies, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Logic t∆∞∆°ng t·ª±: T√¨m a c√≥ path ho·∫∑c trong h4
            links = soup.find_all('a', attrs={'path': True})
            if not links: links = [h4.find('a') for h4 in soup.select('h4') if h4.find('a')]

            for a_tag in links:
                if not a_tag: continue
                path = a_tag.get('path') or a_tag.get('href')
                
                # L·ªåC R√ÅC
                if not path or any(junk in path.lower() for junk in JUNK_DOMAINS): continue
                
                if not path.startswith('http'):
                    full_link = f"{domain}{path}"
                else:
                    full_link = path
                    
                title = a_tag.get_text(strip=True)
                
                # Ng√†y th√°ng
                date_str = str(current_year)
                date_tag = a_tag.find_next_sibling('i')
                if not date_tag and a_tag.parent: date_tag = a_tag.parent.find('i')
                
                if date_tag:
                    raw_date = date_tag.get('date-created') or date_tag.get_text(strip=True)
                    if raw_date:
                        try:
                            clean_date = raw_date[:10].replace('-', '/')
                            d_obj = datetime.strptime(clean_date, "%Y/%m/%d")
                            if d_obj.year != current_year: continue
                            date_str = d_obj.strftime("%d/%m/%Y")
                        except: pass
                
                if full_link in seen_ids: continue
                if any(x['id'] == full_link for x in new_items): continue

                new_items.append({
                    "source": f"VIB - {target['name']}",
                    "id": full_link,
                    "title": title,
                    "date": date_str,
                    "link": full_link
                })

        except Exception as e:
            # print(f"L·ªói VIB {target['name']}: {e}")
            pass

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_ssb_news(seen_ids):
    """
    H√†m c√†o SeABank (SSB).
    - Website d√πng Tailwind CSS.
    - C·∫•u tr√∫c: section.md:block -> a -> div -> h2 (Title).
    - Ng√†y th√°ng: T√¨m text d·∫°ng dd/mm/yyyy g·∫ßn icon l·ªãch.
    """
    
    current_year = str(datetime.now().year)
    domain = "https://www.seabank.com.vn"
    
    configs = [
        {
            "name": "C√¥ng b·ªë th√¥ng tin",
            "url": "https://www.seabank.com.vn/nha-dau-tu/cong-bo-thong-tin"
        },
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://www.seabank.com.vn/nha-dau-tu/bao-cao-tai-chinh"
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": "https://www.seabank.com.vn/nha-dau-tu/dai-hoi-dong-co-dong"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t SSB (NƒÉm {current_year}) ---")

    for cfg in configs:
        try:
            # print(f"   >> ƒêang t·∫£i: {cfg['name']}...")
            response = session.get(cfg['url'], headers=headers, timeout=20, verify=False)
            
            if response.status_code != 200:
                print(f"[SSB] L·ªói k·∫øt n·ªëi {cfg['name']}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. T√¨m c√°c kh·ªëi tin d√†nh cho Desktop (ƒë·ªÉ tr√°nh tr√πng l·∫∑p v·ªõi mobile)
            # Class Tailwind: "hidden md:block" -> d√πng select css selector
            # L∆∞u √Ω d·∫•u : trong css selector ph·∫£i escape ho·∫∑c d√πng attribute selector
            sections = soup.select('section[class*="md:block"]')
            
            if not sections:
                # Fallback: T√¨m th·∫ª a c√≥ href ch·ª©a /nha-dau-tu/
                sections = soup.select(f'a[href^="/nha-dau-tu/"]')

            for item in sections:
                # N·∫øu item l√† section -> t√¨m a con, n·∫øu l√† a -> d√πng lu√¥n
                if item.name == 'a':
                    a_tag = item
                else:
                    a_tag = item.find('a')
                
                if not a_tag: continue
                
                link = a_tag.get('href')
                if not link: continue
                
                # 2. L·∫•y Title (h2)
                title_tag = a_tag.find('h2')
                if not title_tag: continue
                title = title_tag.get_text(strip=True)
                
                # 3. L·∫•y Ng√†y th√°ng
                # C√°ch 1: T√¨m text c√≥ format ng√†y th√°ng trong to√†n b·ªô kh·ªëi
                full_text = a_tag.get_text(" ", strip=True)
                date_str = ""
                
                # Regex t√¨m dd/mm/yyyy
                match = re.search(r'(\d{2}/\d{2}/\d{4})', full_text)
                if match:
                    date_str = match.group(1)
                
                # L·ªçc nƒÉm
                if current_year not in date_str: continue

                # 4. Chu·∫©n h√≥a & L∆∞u
                if not link.startswith('http'):
                    link = f"{domain}{link}"
                
                # Check tr√πng
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue
                
                new_items.append({
                    "source": f"SSB - {cfg['name']}",
                    "id": link,
                    "title": title,
                    "date": date_str,
                    "link": link
                })

            time.sleep(0.5)

        except Exception as e:
            print(f"[SSB] L·ªói ngo·∫°i l·ªá t·∫°i {cfg['name']}: {e}")
            continue

    return new_items

def fetch_tpb_news(seen_ids):
    """
    H√†m c√†o TPBank (TPB) - Selenium Mode.
    - C·∫•u tr√∫c: Web ƒë·ªông, d√πng Selenium ƒë·ªÉ load h·∫øt c√°c block.
    - Parsing: D·ª±a v√†o class 'group-content', 'b-right-download'.
    - Ng√†y th√°ng: Tr√≠ch xu·∫•t tr·ª±c ti·∫øp t·ª´ chu·ªói Title (Regex).
    """
    
    current_year = datetime.now().year
    # current_year = 2025 # Hardcode ƒë·ªÉ test
    
    # 1. C·∫•u h√¨nh danh s√°ch Link c·∫ßn qu√©t
    targets = [
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "https://tpb.vn/nha-dau-tu/bao-cao-tai-chinh"
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": "https://tpb.vn/nha-dau-tu/dai-hoi-dong-co-dong"
        },
        {
            "name": "Th√¥ng b√°o c·ªï ƒë√¥ng",
            "url": "https://tpb.vn/nha-dau-tu/thong-bao-co-dong"
        }
    ]

    # 2. C·∫•u h√¨nh Selenium (Headless)
    chrome_options = Options()
    chrome_options.add_argument("--headless=new") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    new_items = []
    
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t TPB (NƒÉm {current_year}) ---")
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.set_page_load_timeout(30) # TPB load h∆°i l√¢u

    try:
        for target in targets:
            try:
                # print(f"   >> ƒêang t·∫£i: {target['name']}...")
                driver.get(target['url'])
                
                # Ch·ªù 5s ƒë·ªÉ JS ch·∫°y v√† render c√°c block nƒÉm 2025
                time.sleep(5)
                
                # --- LOGIC M·ªû R·ªòNG ACCORDION (QUAN TR·ªåNG) ---
                # TPB th∆∞·ªùng ƒë√≥ng c√°c m·ª•c, ta c·∫ßn click m·ªü nƒÉm hi·ªán t·∫°i n·∫øu n√≥ ch∆∞a m·ªü.
                # Tuy nhi√™n, th∆∞·ªùng nƒÉm m·ªõi nh·∫•t s·∫Ω t·ª± m·ªü. 
                # ƒê·ªÉ ch·∫Øc ƒÉn, ta l·∫•y to√†n b·ªô source sau 5s ch·ªù ƒë·ª£i.
                
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # T√¨m t·∫•t c·∫£ c√°c kh·ªëi n·ªôi dung tin
                # D·ª±a v√†o ·∫£nh: div.group-content
                content_groups = soup.select('.group-content')
                
                count_in_cat = 0
                for group in content_groups:
                    # T√¨m ph·∫ßn b√™n ph·∫£i ch·ª©a link download
                    # D·ª±a v√†o ·∫£nh: div.b_right -> div.b-right-download -> a
                    # C√≥ th·ªÉ c√≥ nhi·ªÅu file trong 1 group (VD: file ti·∫øng Vi·ªát, ti·∫øng Anh)
                    
                    download_divs = group.select('.b-right-download')
                    
                    for div in download_divs:
                        a_tag = div.find('a')
                        if not a_tag: continue
                        
                        link = a_tag.get('href')
                        if not link or "javascript" in link or link == "#": continue
                        
                        # L·∫•y text g·ªëc ƒë·ªÉ t√°ch ng√†y v√† ti√™u ƒë·ªÅ
                        # Text th∆∞·ªùng n·∫±m trong span ho·∫∑c tr·ª±c ti·∫øp trong a
                        # V√≠ d·ª•: " 18/08/2025 B√°o c√°o t√†i ch√≠nh..."
                        full_text = a_tag.get_text(" ", strip=True)
                        
                        # --- X·ª¨ L√ù NG√ÄY TH√ÅNG B·∫∞NG REGEX ---
                        # T√¨m chu·ªói dd/mm/yyyy ·ªü ƒë·∫ßu ho·∫∑c trong text
                        date_match = re.search(r'(\d{2}/\d{2}/\d{4})', full_text)
                        
                        date_str = str(current_year)
                        valid_date = False
                        
                        if date_match:
                            extracted_date = date_match.group(1)
                            try:
                                d_obj = datetime.strptime(extracted_date, "%d/%m/%Y")
                                if d_obj.year == current_year:
                                    date_str = extracted_date
                                    valid_date = True
                            except: pass
                        else:
                            # N·∫øu kh√¥ng th·∫•y ng√†y trong text, check th·ª≠ c√°c class 'year-value' ·ªü block b√™n tr√°i (.b_left)
                            # Nh∆∞ng b·∫°n khuy√™n kh√¥ng n√™n tin t∆∞·ªüng, n√™n ta ∆∞u ti√™n Regex title.
                            # N·∫øu kh√¥ng c√≥ ng√†y trong title -> B·ªè qua ho·∫∑c l·∫•y n·∫øu nghi ng·ªù l√† nƒÉm nay?
                            # An to√†n nh·∫•t: N·∫øu ko c√≥ ng√†y -> B·ªè qua (ƒë·ªÉ tr√°nh l·∫•y tin c≈© t·ª´ c√°c nƒÉm tr∆∞·ªõc l·ªçt v√†o)
                            pass

                        if not valid_date: continue

                        # X·ª≠ l√Ω Ti√™u ƒë·ªÅ: X√≥a ng√†y th√°ng kh·ªèi ti√™u ƒë·ªÅ cho ƒë·∫πp
                        title = full_text.replace(date_str, "").strip()
                        # X√≥a c√°c k√Ω t·ª± th·ª´a nh∆∞ d·∫•u ch·∫•m, g·∫°ch ngang ·ªü ƒë·∫ßu
                        title = re.sub(r'^[\.\-\:\s]+', '', title)
                        
                        if not title: title = "T√†i li·ªáu TPBank"

                        # Chu·∫©n h√≥a Link
                        if not link.startswith('http'):
                            link = f"https://tpb.vn{link}"
                            
                        # Check tr√πng
                        if link in seen_ids: continue
                        if any(x['id'] == link for x in new_items): continue

                        new_items.append({
                            "source": f"TPB - {target['name']}",
                            "id": link,
                            "title": title,
                            "date": date_str,
                            "link": link
                        })
                        count_in_cat += 1

                # print(f"      -> T√¨m th·∫•y {count_in_cat} tin.")

            except Exception as e:
                print(f"[TPB] L·ªói t·∫°i {target['name']}: {e}")
                continue

    except Exception as e:
        print(f"[TPB] L·ªói Driver: {e}")
    finally:
        driver.quit()
        
    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_vea_news(seen_ids):
    """
    H√†m c√†o VEAM (VEA).
    - Fix l·ªói l·∫•y nh·∫ßm th·∫ª Title c·ªßa ·∫£nh (r·ªóng).
    - Selector chu·∫©n: .text-box-news > a.title-new
    """
    
    current_year = str(datetime.now().year)
    domain = "http://veamcorp.com"
    
    configs = [
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": "http://veamcorp.com/tin-tuc/bao-cao-tai-chinh-113.html"
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": "http://veamcorp.com/tin-tuc/dai-hoi-dong-co-dong-118.html"
        },
        {
            "name": "C√¥ng b·ªë th√¥ng tin",
            "url": "http://veamcorp.com/tin-tuc/cong-bo-thong-tin-114.html"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('http://', LegacySSLAdapter())
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t VEAM (NƒÉm {current_year}) ---")

    for cfg in configs:
        try:
            response = session.get(cfg['url'], headers=headers, timeout=20)
            response.encoding = 'utf-8' # √âp m√£ h√≥a
            
            if response.status_code != 200:
                print(f"[VEA] L·ªói k·∫øt n·ªëi {cfg['name']}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m c√°c kh·ªëi tin
            items = soup.select('.box-catnew')
            
            for item in items:
                # --- FIX QUAN TR·ªåNG T·∫†I ƒê√ÇY ---
                # Ch·ªâ t√¨m th·∫ª a.title-new n·∫±m trong kh·ªëi .text-box-news
                # (Tr√°nh l·∫•y nh·∫ßm th·∫ª a.title-new bao quanh ·∫£nh ·ªü b√™n tr√°i)
                title_tag = item.select_one('.text-box-news a.title-new')
                
                if not title_tag: continue
                
                title = title_tag.get_text(" ", strip=True)
                link = title_tag.get('href')
                
                if not link: continue
                
                # Fallback: N·∫øu v·∫´n r·ªóng, th·ª≠ l·∫•y t·ª´ thu·ªôc t√≠nh title (n·∫øu c√≥)
                if not title: title = title_tag.get('title', 'T√†i li·ªáu VEAM')

                # 2. L·∫§Y NG√ÄY
                date_div = item.select_one('.text-date-new')
                date_str = ""
                
                if date_div:
                    raw_text = date_div.get_text(strip=True) # "Ng√†y ƒëƒÉng: 25/11/2025"
                    clean_text = raw_text.replace("Ng√†y ƒëƒÉng:", "").strip()
                    
                    if current_year in clean_text:
                        date_str = clean_text
                
                if not date_str: continue

                # 3. CHU·∫®N H√ìA LINK
                if not link.startswith('http'):
                    link = f"http://veamcorp.com{link}"
                
                # 4. CHECK TR√ôNG
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue
                
                new_items.append({
                    "source": f"VEA - {cfg['name']}",
                    "id": link,
                    "title": title,
                    "date": date_str,
                    "link": link
                })

            time.sleep(0.5)

        except Exception as e:
            print(f"[VEA] L·ªói ngo·∫°i l·ªá t·∫°i {cfg['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# --- C·∫§U H√åNH SSL FIX ---
class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_fox_news(seen_ids):
    """
    H√†m c√†o FPT Telecom (FOX) - Phi√™n b·∫£n L·ªçc Ti·∫øng Anh.
    - S·ª≠ d·ª•ng tham s·ªë ?tag={year} ƒë·ªÉ l·ªçc server-side.
    - Lo·∫°i b·ªè c√°c tin c√≥ ti√™u ƒë·ªÅ ch·ª©a "Ti·∫øng Anh" ho·∫∑c "English".
    """
    
    current_year = str(datetime.now().year)
    
    configs = [
        {
            "name": "B√°o c√°o t√†i ch√≠nh",
            "url": f"https://fpt.vn/vi/ve-fpt-telecom/quan-he-co-dong/bao-cao-tai-chinh/?tag={current_year}"
        },
        {
            "name": "ƒê·∫°i h·ªôi ƒë·ªìng c·ªï ƒë√¥ng",
            "url": f"https://fpt.vn/vi/ve-fpt-telecom/quan-he-co-dong/dai-hoi-co-dong-fpt-telecom/?tag={current_year}"
        },
        {
            "name": "Th√¥ng b√°o tr·∫£ c·ªï t·ª©c",
            "url": f"https://fpt.vn/vi/ve-fpt-telecom/quan-he-co-dong/thong-bao-tra-co-tuc/?tag={current_year}"
        }
    ]
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t FOX (NƒÉm {current_year}) ---")

    for cfg in configs:
        try:
            response = session.get(cfg['url'], headers=headers, timeout=20, verify=False)
            
            if response.status_code != 200:
                print(f"[FOX] L·ªói k·∫øt n·ªëi {cfg['name']}: {response.status_code}")
                continue

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # T√¨m t·∫•t c·∫£ c√°c d√≤ng d·ªØ li·ªáu (tr.table-row)
            rows = soup.select('tr.table-row')
            
            if not rows:
                continue

            for row in rows:
                # L·∫•y c√°c c·ªôt
                cols = row.find_all('td')
                if len(cols) < 2: continue
                
                # 1. L·∫§Y TI√äU ƒê·ªÄ (C·ªôt 0)
                title = cols[0].get_text(strip=True)
                
                # --- LOGIC L·ªåC TI·∫æNG ANH (M·ªöI TH√äM) ---
                title_lower = title.lower()
                if "ti·∫øng anh" in title_lower or "english" in title_lower:
                    # B·ªè qua ngay l·∫≠p t·ª©c
                    continue

                # 2. L·∫§Y NG√ÄY (C·ªôt 1)
                # Format: 24-10-2025 16:54
                date_text = cols[1].get_text(strip=True)
                date_str = date_text.split(" ")[0] if " " in date_text else date_text
                
                # Check nƒÉm
                if current_year not in date_str:
                    continue

                # 3. L·∫§Y LINK T·∫¢I
                link_tag = row.select_one('a.img-download')
                if not link_tag:
                    link_tag = row.select_one('a.view-pdf')
                
                if not link_tag: continue
                
                link = link_tag.get('href')
                if not link: continue

                # Chu·∫©n h√≥a Link
                if not link.startswith('http'):
                    link = f"https://fpt.vn{link}"

                # 4. CHECK TR√ôNG & L∆ØU
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue
                
                new_items.append({
                    "source": f"FOX - {cfg['name']}",
                    "id": link,
                    "title": title,
                    "date": date_str,
                    "link": link
                })

            time.sleep(0.5)

        except Exception as e:
            print(f"[FOX] L·ªói ngo·∫°i l·ªá t·∫°i {cfg['name']}: {e}")
            continue

    return new_items

# T·∫Øt c·∫£nh b√°o SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class LegacySSLAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl_.create_urllib3_context()
        ctx.options |= 0x4 
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections, maxsize=maxsize, block=block, ssl_context=ctx
        )

def fetch_gex_news(seen_ids):
    """
    H√†m c√†o Gelex (GEX) - Fix Selector B·∫£ng & List.
    """
    
    current_year = datetime.now().year
    # current_year = 2025 # M·ªü d√≤ng n√†y ƒë·ªÉ test n·∫øu c·∫ßn
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    new_items = []
    session = requests.Session()
    session.mount('https://', LegacySSLAdapter())

    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t GEX (Fixed Selector - NƒÉm {current_year}) ---")

    # ==========================================================
    # PH·∫¶N 1: B√ÅO C√ÅO T√ÄI CH√çNH (X·ª≠ l√Ω B·∫£ng ph·ª©c t·∫°p)
    # ==========================================================
    url_bctc = "https://gelex.vn/doc-cat/bao-cao-tai-chinh"
    
    try:
        response = session.get(url_bctc, headers=headers, timeout=20, verify=False)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # FIX: T√¨m div wrapper tr∆∞·ªõc, sau ƒë√≥ t√¨m table b√™n trong
        # -> div.report-table -> table
        wrapper = soup.select_one('.report-table')
        if wrapper:
            table = wrapper.find('table')
            if table:
                rows = table.find_all('tr')
                
                # Bi·∫øn tr·∫°ng th√°i ƒë·ªÉ nh·ªõ ti√™u ƒë·ªÅ c·ªßa nh√≥m hi·ªán t·∫°i
                # VD: ƒêang duy·ªát nh√≥m "B√°o c√°o t√†i ch√≠nh" -> "B√°o c√°o Ri√™ng"
                current_group = "BCTC"
                current_sub = ""
                
                for row in rows:
                    # 1. C·∫≠p nh·∫≠t Ti√™u ƒë·ªÅ Nh√≥m (Class 'parent')
                    parent_td = row.find('td', class_='parent')
                    if parent_td:
                        current_group = parent_td.get_text(strip=True)
                        current_sub = "" # Reset sub khi sang nh√≥m m·ªõi
                    
                    # 2. C·∫≠p nh·∫≠t Ti√™u ƒë·ªÅ Con (Class 'quatar' - l·ªói ch√≠nh t·∫£ c·ªßa GEX, ho·∫∑c 'child')
                    # T√¨m td c√≥ class ch·ª©a 'child' ho·∫∑c 'quatar'
                    sub_td = row.find('td', class_=lambda x: x and ('child' in x or 'quatar' in x))
                    if sub_td:
                        # Ch·ªâ l·∫•y text n·∫øu td n√†y KH√îNG ch·ª©a file download (ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n)
                        if not sub_td.find('div', class_='report-table-item'):
                            text = sub_td.get_text(strip=True)
                            if text: current_sub = text
                    
                    # 3. T√¨m c√°c √¥ ch·ª©a file (report-table-item)
                    file_items = row.select('.report-table-item')
                    
                    for item in file_items:
                        # L·∫•y Ng√†y: <div class="date-pdf">22/04/2025</div>
                        date_tag = item.select_one('.date-pdf')
                        if not date_tag: continue
                        
                        date_str = date_tag.get_text(strip=True)
                        try:
                            d_obj = datetime.strptime(date_str, "%d/%m/%Y")
                            if d_obj.year != current_year: continue
                        except: continue

                        # L·∫•y Link
                        a_tag = item.find('a')
                        if not a_tag: continue
                        link = a_tag.get('href')
                        if not link: continue
                        
                        # T·∫°o ti√™u ƒë·ªÅ th√¥ng minh
                        # VD: B√°o c√°o t√†i ch√≠nh - B√°o c√°o Ri√™ng (22/04/2025)
                        full_title = f"{current_group}"
                        if current_sub:
                            full_title += f" - {current_sub}"
                        full_title += f" ({date_str})"
                        
                        if link in seen_ids: continue
                        if any(x['id'] == link for x in new_items): continue

                        new_items.append({
                            "source": "GEX - BCTC",
                            "id": link,
                            "title": full_title,
                            "date": date_str,
                            "link": link
                        })

    except Exception as e:
        print(f"[GEX] L·ªói BCTC: {e}")


    # ==========================================================
    # PH·∫¶N 2: DANH S√ÅCH (CBTT & ƒêHƒêCƒê)
    # ==========================================================
    list_targets = [
        {"name": "CBTT", "url": "https://gelex.vn/doc-cat/cong-bo-thong-tin-2"},
        {"name": "ƒêHƒêCƒê", "url": "https://gelex.vn/doc-cat/tai-lieu-dai-hoi-dong-cd"}
    ]

    for target in list_targets:
        try:
            response = session.get(target['url'], headers=headers, timeout=20, verify=False)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # FIX: T√¨m th·∫ª li cha (li-report-list) cho ch·∫Øc ch·∫Øn
            #
            list_items = soup.select('li.li-report-list')
            
            # print(f"   > {target['name']}: T√¨m th·∫•y {len(list_items)} m·ª•c.")
            
            for li in list_items:
                # 1. L·∫•y Ng√†y (.meta)
                date_tag = li.select_one('.meta')
                if not date_tag: continue
                
                date_str = date_tag.get_text(strip=True) # VD: 19/11/2025
                try:
                    d_obj = datetime.strptime(date_str, "%d/%m/%Y")
                    if d_obj.year != current_year: continue
                except: continue

                # 2. L·∫•y Ti√™u ƒë·ªÅ & Link (.li-report-item-title-link)
                # ∆Øu ti√™n l·∫•y title link (ch·ª©a text ti√™u ƒë·ªÅ)
                # L∆∞u √Ω: C√≥ th·ªÉ c√≥ 2 th·∫ª a (1 c√°i l√† icon download, 1 c√°i l√† text). Ta l·∫•y c√°i text.
                # C√°ch ph√¢n bi·ªát: class icon download th∆∞·ªùng l√† 'li-report-item-title-link-download'
                
                title_link = li.select_one('a.li-report-item-title-link')
                if not title_link: continue
                
                title = title_link.get_text(strip=True)
                link = title_link.get('href')
                
                if not link: continue
                
                # Check tr√πng
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": f"GEX - {target['name']}",
                    "id": link,
                    "title": title,
                    "date": date_str,
                    "link": link
                })

        except Exception as e:
            print(f"[GEX] L·ªói {target['name']}: {e}")

    return new_items

def fetch_eib_news(seen_ids):
    """
    H√†m c√†o Eximbank (EIB).
    - Trang web s·ª≠ d·ª•ng Next.js + Tailwind CSS.
    - D·ªØ li·ªáu n·∫±m trong c√°c div c√≥ ID s·ªë (vd: id="810", id="795").
    - C√≥ b·ªô l·ªçc ng√¥n ng·ªØ (B·ªè b·∫£n Ti·∫øng Anh).
    """
    
    current_year = datetime.now().year
    # current_year = 2025 # M·ªü d√≤ng n√†y ƒë·ªÉ test gi·∫£ l·∫≠p nƒÉm 2025
    
    # C·∫•u h√¨nh Selenium
    chrome_options = Options()
    chrome_options.add_argument("--headless=new") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    new_items = []
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t EIB (NƒÉm {current_year}) ---")
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.set_page_load_timeout(30)

    # Danh s√°ch link c·∫ßn c√†o
    targets = [
        {"name": "BCTC", "url": "https://eximbank.com.vn/bao-cao-tai-chinh"},
        {"name": "ƒêHƒêCƒê", "url": "https://eximbank.com.vn/dai-hoi-dong-co-dong"}
    ]

    try:
        for target in targets:
            # print(f"   >> ƒêang t·∫£i: {target['name']}...")
            driver.get(target['url'])
            
            # Cu·ªôn trang ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu load h·∫øt (React hay lazy load)
            for _ in range(3):
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1)
            time.sleep(3) # Ch·ªù render cu·ªëi c√πng
            
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # --- CHI·∫æN THU·∫¨T: T√åM C√ÅC KH·ªêI TIN THEO ID S·ªê ---
            # D·ª±a v√†o ·∫£nh: <div id="810">, <div id="795">...
            # Ta t√¨m t·∫•t c·∫£ div c√≥ thu·ªôc t√≠nh id, v√† id ƒë√≥ ph·∫£i l√† s·ªë
            # Ho·∫∑c t√¨m div ch·ª©a class "flex flex-col" (Container ch√≠nh c·ªßa EIB)
            
            # C√°ch an to√†n nh·∫•t: T√¨m t·∫•t c·∫£ th·∫ª <a> c√≥ thu·ªôc t√≠nh download ho·∫∑c href ch·ª©a .pdf
            all_links = soup.select('a[href$=".pdf"]')
            
            # N·∫øu kh√¥ng t√¨m th·∫•y theo ƒëu√¥i pdf, t√¨m theo th·∫ª a c√≥ attribute 'download'
            if not all_links:
                all_links = soup.select('a[download]')

            for a_tag in all_links:
                link = a_tag.get('href')
                if not link: continue
                
                # L·∫•y text ti√™u ƒë·ªÅ (th∆∞·ªùng n·∫±m trong th·∫ª a ho·∫∑c th·∫ª p con)
                raw_text = a_tag.get_text(" ", strip=True)
                
                # --- 1. L·ªåC TI·∫æNG ANH ---
                # B·ªè qua n·∫øu t√™n file ho·∫∑c ti√™u ƒë·ªÅ c√≥ d·∫•u hi·ªáu ti·∫øng Anh
                lower_text = raw_text.lower()
                lower_link = link.lower()
                
                keywords_eng = ["financial statement", "eng.pdf", "- en", "english", "resolution", 'eng']
                if any(kw in lower_link for kw in keywords_eng) or any(kw in lower_text for kw in keywords_eng):
                    # print(f"      -> B·ªè qua b·∫£n Ti·∫øng Anh: {raw_text[:30]}...")
                    continue

                # --- 2. T√åM NG√ÄY TH√ÅNG ---
                date_str = str(current_year)
                found_date = False
                
                # Chi·∫øn thu·∫≠t t√¨m ng√†y:
                # C√°ch A (BCTC): Ng√†y n·∫±m trong ti√™u ƒë·ªÅ cha (div id="810" -> p)
                # VD: "B√°o c√°o t√†i ch√≠nh Qu√Ω 3 nƒÉm 2025 (30/10/2025)"
                # Ta ph·∫£i leo ng∆∞·ª£c l√™n t√¨m container cha
                
                # C√°ch B (ƒêHƒêCƒê): Ng√†y n·∫±m trong th·∫ª <p> anh em v·ªõi ti√™u ƒë·ªÅ b√™n trong th·∫ª <a>
                # VD: <p>29/04/2025</p>
                
                # Th·ª≠ t√¨m ng√†y trong ch√≠nh text c·ªßa th·∫ª a tr∆∞·ªõc
                date_match = re.search(r'(\d{2}/\d{2}/\d{4})', raw_text)
                if date_match:
                    date_str = date_match.group(1)
                    found_date = True
                
                # N·∫øu kh√¥ng th·∫•y, leo l√™n cha ƒë·ªÉ t√¨m (Cho tr∆∞·ªùng h·ª£p BCTC)
                if not found_date:
                    parent = a_tag.find_parent(id=True) # T√¨m div cha c√≥ ID (nh∆∞ id="810")
                    if parent:
                        # T√¨m th·∫ª p ƒë·∫ßu ti√™n trong block n√†y (th∆∞·ªùng l√† ti√™u ƒë·ªÅ nh√≥m ch·ª©a ng√†y)
                        header_p = parent.find('p')
                        if header_p:
                            header_text = header_p.get_text(strip=True)
                            date_match_parent = re.search(r'(\d{2}/\d{2}/\d{4})', header_text)
                            if date_match_parent:
                                date_str = date_match_parent.group(1)
                                found_date = True
                
                # --- 3. KI·ªÇM TRA NƒÇM ---
                try:
                    d_obj = datetime.strptime(date_str, "%d/%m/%Y")
                    if d_obj.year != current_year:
                        continue # B·ªè qua nƒÉm c≈©
                except:
                    # N·∫øu kh√¥ng parse ƒë∆∞·ª£c ng√†y, ki·ªÉm tra xem link c√≥ ch·ª©a "2025" kh√¥ng
                    if str(current_year) not in link and str(current_year) not in raw_text:
                        continue

                # --- 4. CHU·∫®N H√ìA TI√äU ƒê·ªÄ ---
                # N·∫øu text qu√° d√†i ho·∫∑c ch·ª©a ng√†y, l√†m s·∫°ch
                title = raw_text.replace(date_str, "").strip()
                title = re.sub(r'\s+', ' ', title) # X√≥a kho·∫£ng tr·∫Øng th·ª´a
                if len(title) < 5: title = "T√†i li·ªáu Eximbank"

                # Check tr√πng
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": f"EIB - {target['name']}",
                    "id": link,
                    "title": title,
                    "date": date_str,
                    "link": link
                })

    except Exception as e:
        print(f"[EIB] L·ªói: {e}")
    finally:
        driver.quit()
        
    return new_items

def fetch_msb_news(seen_ids):
    """
    H√†m c√†o MSB (Maritime Bank) - Phi√™n b·∫£n Selenium.
    - L√Ω do: D·ªØ li·ªáu ƒë∆∞·ª£c render b·∫±ng JS (Liferay), requests kh√¥ng l·∫•y ƒë∆∞·ª£c.
    - Chi·∫øn thu·∫≠t: Ch·ªù class .baocao-item xu·∫•t hi·ªán r·ªìi m·ªõi c√†o.
    """
    
    current_year = datetime.now().year
    # current_year = 2025 # M·ªü d√≤ng n√†y ƒë·ªÉ test
    
    # Danh s√°ch link c·∫ßn c√†o
    targets = [
        {"name": "ƒêHƒêCƒê", "url": "https://www.msb.com.vn/vi/nha-dau-tu/dai-hoi-dong-co-dong.html"},
        {"name": "BCTC", "url": "https://www.msb.com.vn/vi/nha-dau-tu/bao-cao-tai-chinh.html"}
    ]
    
    # C·∫•u h√¨nh Selenium
    chrome_options = Options()
    chrome_options.add_argument("--headless=new") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    new_items = []
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t MSB (Selenium Mode - NƒÉm {current_year}) ---")
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.set_page_load_timeout(30)

    try:
        for target in targets:
            # print(f"   >> ƒêang t·∫£i: {target['name']}...")
            driver.get(target['url'])
            
            # 1. CH·ªú D·ªÆ LI·ªÜU XU·∫§T HI·ªÜN
            # Ch·ªù t·ªëi ƒëa 15s cho ƒë·∫øn khi class 'baocao-item' xu·∫•t hi·ªán
            try:
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "baocao-item"))
                )
                # Ch·ªù th√™m 2s ƒë·ªÉ c√°c script render ho√†n t·∫•t h·∫≥n
                time.sleep(2)
            except:
                print(f"   ! Timeout: Kh√¥ng th·∫•y d·ªØ li·ªáu t·∫°i {target['name']}")
                continue

            # 2. PARSE HTML ƒê√É RENDER
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            items = soup.select('.baocao-item')
            
            # print(f"      -> T√¨m th·∫•y {len(items)} m·ª•c.")
            
            for item in items:
                # --- L·∫§Y NG√ÄY (Logic c≈© - v·∫´n ƒë√∫ng v·ªõi Element) ---
                p_tag = item.find('p')
                if not p_tag: continue
                
                raw_date_text = p_tag.get_text(" ", strip=True) 
                date_match = re.search(r'(\d{2}/\d{2}/\d{4})', raw_date_text)
                
                date_str = str(current_year)
                if date_match:
                    extracted_date = date_match.group(1)
                    try:
                        d_obj = datetime.strptime(extracted_date, "%d/%m/%Y")
                        if d_obj.year != current_year:
                            continue # B·ªè qua nƒÉm c≈©
                        date_str = extracted_date
                    except: continue
                else:
                    continue

                # --- L·∫§Y TI√äU ƒê·ªÄ & LINK ---
                h3_tag = item.find('h3')
                if not h3_tag: continue
                title = h3_tag.get_text(strip=True)

                a_tag = item.find('a')
                if not a_tag: continue
                link = a_tag.get('href')
                
                if not link: continue
                if not link.startswith('http'):
                    link = f"https://www.msb.com.vn{link}"

                # Check tr√πng
                if link in seen_ids: continue
                if any(x['id'] == link for x in new_items): continue

                new_items.append({
                    "source": f"MSB - {target['name']}",
                    "id": link,
                    "title": title,
                    "date": date_str,
                    "link": link
                })

    except Exception as e:
        print(f"[MSB] L·ªói Selenium: {e}")
    finally:
        driver.quit()
        
    return new_items

def fetch_bvh_news(seen_ids):
    """
    H√†m c√†o BVH - Phi√™n b·∫£n Final (ƒê√£ fix c·∫•u tr√∫c CBTT f-panel).
    """
    current_year = datetime.now().year
    # current_year = 2025 # M·ªü d√≤ng n√†y ƒë·ªÉ test
    
    url = "https://baoviet.com.vn/vi/quan-he-co-dong"
    
    chrome_options = Options()
    chrome_options.add_argument("--headless=new") 
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    new_items = []
    print(f"--- üöÄ B·∫Øt ƒë·∫ßu qu√©t BVH (V4 Final - NƒÉm {current_year}) ---")
    
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.set_page_load_timeout(30)

    try:
        driver.get(url)
        time.sleep(3)

        # ==================================================================
        # 1. X·ª¨ L√ù C√îNG B·ªê TH√îNG TIN (CBTT) - B·ªô l·ªçc tr√™n
        # ==================================================================
        # print("   >> [1/2] ƒêang x·ª≠ l√Ω CBTT...")
        try:
            # --- FILTER NƒÇM ---
            # T√¨m √¥ ch·ªçn nƒÉm CBTT (ko c√≥ ƒëu√¥i --2)
            nice_select_cbtt = driver.find_element(By.CSS_SELECTOR, "div.js-form-item-field-doc-year .nice-select")
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", nice_select_cbtt)
            time.sleep(1)
            driver.execute_script("arguments[0].click();", nice_select_cbtt)
            time.sleep(1)
            
            # Ch·ªçn nƒÉm
            options = nice_select_cbtt.find_elements(By.CSS_SELECTOR, "ul.list li")
            found_year_cbtt = False
            for opt in options:
                if str(current_year) in opt.get_attribute("innerText"):
                    driver.execute_script("arguments[0].click();", opt)
                    found_year_cbtt = True
                    break
            
            if found_year_cbtt:
                # B·∫•m Apply (ID: edit-submit-document-report)
                apply_btn = driver.find_element(By.ID, "edit-submit-document-report")
                driver.execute_script("arguments[0].click();", apply_btn)
                time.sleep(5) # Ch·ªù reload
            
            # --- C√ÄO D·ªÆ LI·ªÜU CBTT (LOGIC M·ªöI) ---
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            
            # T√¨m container l·ªõn bao quanh c√°c accordion
            cbtt_container = soup.select_one('.tlBCao-table')
            
            if cbtt_container:
                # T√¨m t·∫•t c·∫£ th·∫ª f-panel (D√π n·∫±m trong accordion ƒë√≥ng hay m·ªü th√¨ source v·∫´n c√≥)
                items = cbtt_container.select('.f-panel')
                
                # print(f"      -> T√¨m th·∫•y {len(items)} tin CBTT.")
                
                for item in items:
                    # 1. Ti√™u ƒë·ªÅ: h3.post__title
                    #
                    title_tag = item.select_one('.post__title')
                    if not title_tag: continue
                    title = title_tag.get_text(strip=True)
                    
                    # 2. Link: a.btn-link
                    #
                    link_tag = item.select_one('a.btn-link')
                    if not link_tag: continue
                    link = link_tag.get('href')
                    
                    if not link: continue
                    if not link.startswith('http'): link = f"https://baoviet.com.vn{link}"
                    
                    # 3. Ng√†y: p.post__date -> time
                    date_str = str(current_year)
                    time_tag = item.select_one('.post__date time')
                    if time_tag:
                        try:
                            # Text d·∫°ng: 01.12.2025
                            raw_date = time_tag.get_text(strip=True)
                            d = datetime.strptime(raw_date, "%d.%m.%Y")
                            if d.year == current_year:
                                date_str = d.strftime("%d/%m/%Y")
                            else:
                                continue # B·ªè qua nƒÉm c≈©
                        except: pass

                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue
                    
                    new_items.append({
                        "source": "BVH - CBTT",
                        "id": link, "title": title, "date": date_str, "link": link
                    })
        except Exception as e:
            print(f"   ! L·ªói ph·∫ßn CBTT: {e}")

        # ==================================================================
        # 2. X·ª¨ L√ù B√ÅO C√ÅO T√ÄI CH√çNH (BCTC) - B·ªô l·ªçc d∆∞·ªõi (ƒê√£ ch·∫°y OK)
        # ==================================================================
        # print("   >> [2/2] ƒêang x·ª≠ l√Ω BCTC...")
        try:
            # T√¨m √¥ select BCTC (C√ì ƒëu√¥i --2)
            bctc_nice_select = driver.find_element(By.XPATH, "//select[@id='edit-field-doc-year--2']/following-sibling::div[contains(@class, 'nice-select')]")
            
            driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", bctc_nice_select)
            time.sleep(1)
            driver.execute_script("arguments[0].click();", bctc_nice_select)
            time.sleep(1)
            
            options_bctc = bctc_nice_select.find_elements(By.CSS_SELECTOR, "ul.list li")
            found_year_bctc = False
            for opt in options_bctc:
                if str(current_year) in opt.get_attribute("innerText"):
                    driver.execute_script("arguments[0].click();", opt)
                    found_year_bctc = True
                    break
            
            if found_year_bctc:
                apply_btn_bctc = driver.find_element(By.ID, "edit-submit-document-report--2")
                driver.execute_script("arguments[0].click();", apply_btn_bctc)
                time.sleep(5)
            
            # --- C√ÄO D·ªÆ LI·ªÜU BCTC ---
            soup = BeautifulSoup(driver.page_source, 'html.parser')
            # Container: view-display-id-bao_cao_tai_chinh_block
            bctc_block = soup.select_one('.view-display-id-bao_cao_tai_chinh_block')
            
            if bctc_block:
                links = bctc_block.select('ul.item-list li a')
                for a_tag in links:
                    link = a_tag.get('href')
                    title = a_tag.get_text(strip=True)
                    
                    if not link or not title: continue
                    if not link.startswith('http'): link = f"https://baoviet.com.vn{link}"
                    
                    # BCTC kh√¥ng c√≥ ng√†y c·ª• th·ªÉ, l·ªçc theo Text Title
                    if str(current_year) not in title and str(current_year - 1) in title:
                        continue
                        
                    if link in seen_ids: continue
                    if any(x['id'] == link for x in new_items): continue

                    new_items.append({
                        "source": "BVH - BCTC",
                        "id": link, "title": title, "date": str(current_year), "link": link
                    })

        except Exception as e:
            # print(f"   ! L·ªói ph·∫ßn BCTC: {e}")
            pass

    except Exception as e:
        print(f"[BVH] L·ªói Selenium: {e}")
    finally:
        driver.quit()
        
    return new_items